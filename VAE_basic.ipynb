{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "VAE basic.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNF+Sspjp+pju6tSivFjdZH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/akshatvishu/DL-notes/blob/main/VAE_basic.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "kP7QnGyZDDl6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Assume we have a training data $(x^{i})_{i=1}^{N}$ is generated from an latent representation z.\n",
        "\n",
        "Intution: \n",
        "\n",
        "x = image, \n",
        "z is latent factor used to generate x:attributes,orientation etc.\n",
        "\n",
        "\n",
        "1. Learn latent features z from the raw data\n",
        "2. Sample fromt the model to generate new data\n",
        "3. Assume simple prior p(z), eg: gaussian\n",
        "4. Represent p(x|z) with a NN.\n",
        "\n",
        "After training, sample new data :\n",
        "\n",
        "$$p_Θ*(x|z^{(i)})$$\n",
        "\n",
        "* sample z from prior $p_Θ*(z)$\n",
        "\n",
        "* During training:-\n",
        "z -----> x\n",
        "\n",
        "\n",
        "\n",
        "Decoder must be probabilistic:\n",
        "\n",
        "* Decoder inputs z, outputs mean $μ_{x|z}$ and diagonal covariance $∑_{x|z}$ .\n",
        "\n",
        "* Sampel x from gaussian with mean $μ_{x|z} $ and diagonal covariance $ ∑_{x|z} $\n",
        "\n",
        "\n",
        "\n",
        "How to train model?\n",
        "Basic Idea: Max. Likehood of data\n",
        "If we could observe z for each x, then we could train a conditional generative model p(x|z) .\n",
        "\n",
        "\n",
        "We don't observe z, so we need to marginalize it\n",
        "Joint distribution over z and x and then we can further broken it down to condtional prob. of x given z times the prior distribution of z (using chain rule) and this prior we assume that is gaussian. But the integral is intractable to compute as z is some higher dimension vector space .\n",
        "\n",
        "Formula : Density over x\n",
        "$$p_Θ(x)=∫p_Θ(x,z)dz=∫p_Θ(x|z)p_Θ(z)dz $$\n",
        "\n",
        "\n",
        "Thus, let's use Bayes' Rule:\n",
        "$$ p_θ(x)=\\frac{p_θ(x|z)p_θ(z)}{p_θ(z|x)} $$\n",
        "\n",
        "\n",
        "But here still we can't calculate $p_θ(z|x)$ \n",
        "\n",
        "Solution: Train another networ (encoder) that learns \n",
        "\n",
        "$$ \n",
        "q_Φ(z|x)≈p_θ(z|x) \n",
        "$$\n",
        "Thus,\n",
        "\n",
        "\n",
        "$$ \n",
        "\\mathbb p_θ(x)=\\frac{p_θ(x|z)p_θ(z)}{p_θ(z|x)} ≈  p_θ(x)=\\frac{p_θ(x|z)p_θ(z)}{q_Φ(z|x)}\n",
        " $$\n",
        "\n",
        "Encoder Network inputs data x, gives distribution over latent codes z\n",
        "\n",
        "$$\n",
        "\\mathbb q_Φ(z|x)= N(μ_{z|x},∑_{z|x}) \n",
        "$$\n",
        "\n",
        "Decoder Network inputs latent code z, gives distribution over data x\n",
        "\n",
        "$$\n",
        "\\mathbb p_θ(x|z)= N(μ_{x|z},∑_{x|z}) \n",
        "$$\n",
        "\n",
        "Idea: Jointly train both encoder and decoder\n",
        "\n",
        "$$\n",
        "\\mathbb \\log p_θ(x) = \\log \\frac {p_θ(x|z)p(z)}{p_θ(z|x)} = \\log \\frac {p_θ(x|z)p(z)q_Φ(z|x)}{p_θ(z|x)q_Φ(z|x)}\n",
        "$$\n",
        "\n",
        "Multiply top and bottom by $q_Φ(z|x)$\n",
        "\n",
        "Using logarithms to split up the terms:\n",
        "\n",
        "$$\n",
        "\\mathbb =\\log p_θ(x|z) - \\log\\frac{q_Φ(z|x)}{p(z)}+\\log\\frac{q_Φ(z|x)}{p_θ(z|x)}\n",
        "$$\n",
        "\n",
        "Now, we can see that $\\log p_θ(x)$ does not depend upon z.Also, whenever we have a random variable that isn't dependent on another random variable then we can take the expectation over that whole thing.\n",
        "\n",
        "$$\n",
        "\\mathbb \\log p_θ(x)=\\mathbb{E}_{z \\sim q_Φ(z|x)}\\left[\\log p_θ(x)\\right]\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\mathbb \\log p_θ(x)=\\mathbb {E_z}\\left[\\log p_θ(x|z)\\right]-\\mathbb{E}_{z}\\left[\\log \\frac{q_Φ(z|x)}{p(z)}\\right]+\\mathbb{E}_{z}\\left[\\log \\frac{q_Φ(z|x)}{p_θ(z|x)}\\right]\n",
        "$$\n",
        "\n",
        "\n",
        "$$\n",
        "=\\color{yellow}{\\mathbb{E}_{z \\sim q_Φ(z|x)}\\left[\\log p_θ(x|z)\\right]}-\\color{green}{\\mathbb{D_{KL}\\left(q_Φ(z|x),p(z)\\right)}+\\color{red}{\\mathbb{D_{KL}\\left(q_Φ(z|x),p_θ(z|x)\\right)}}}\n",
        "$$\n",
        "\n",
        "* Yellow - data reconstruction\n",
        "* Green  - KL divergence b/w prior and samples from the encoder network\n",
        "* Red    - KL divergence b/w encoder and posterior of decoder \n",
        "\n",
        "Now, the red term is the one we cannot compute becuase it involves $p_θ(z|x)$ but we can compute the yellow and the green terms. Also, we know that KL >=0, so dropping this last red term gives a **lower bound** on the data likelihood:\n",
        "\n",
        "$$\n",
        "\\mathbb \\log p_θ(x) \\geq \\color{yellow}{\\mathbb{E}_{z \\sim q_Φ(z|x)}\\left[\\log p_θ(x|z)\\right]}-\\color{green}{\\mathbb{D_{KL}\\left(q_Φ(z|x),p(z)\\right)}}\n",
        "$$\n",
        "\n",
        "On the left, we have the actual true density of the data under the probalistic model that we setup and on the right we have the lower bound to that density that contains a reconstruction term and a KL divergence term.\n",
        "\n",
        "Thus, we jointly want to train encoder q and decoder p to maximize the **variational lower bound** on the data likehood(Trick:Variationl Inference).\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "bgJ-KoFhRNgA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "hqYA4hf8DDxS"
      }
    }
  ]
}